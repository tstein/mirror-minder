#!/usr/bin/env python
"""Continuously monitor Termux repo mirrors for freshness. Files Github issues if the
data being vended by a mirror falls too far behind the authoritative mirrors we
initially push to, as well as if we are unable to confirm that a mirror is up to date
due to durable availability or parsing issues.

Determines the list of repos to monitor on startup and holds them forever, and does not
cache any info about mirror freshness between runs.

Major TODOs:
  * Alert if this program's is unable to maintain current knowledge of the authoritative
    repos.
  * Alert on logic bugs.
Potentially worthwhile upgrades:
  * Handle updates to the mirror list mid-run.
  * Auto-resolve the issues it creates when possible.
  * Cache knowledge of repos between runs to reduce the time to first useful work after
    a restart. Not really necessarily on the timelines where getting these alerts are
    important.
"""

import argparse
import logging
import os
import os.path
import random
import time
import urllib3
from dataclasses import dataclass
from datetime import datetime, timedelta, UTC
from typing import Optional

import requests
import sh
from sh import git

from issues import issue_body, issue_title, open_new_issue, search_issues

############################
# hard-coded configuration #
############################
TERMUX_TOOLS_REPO = "termux-tools"
# Approximately how long to wait after startup before the initial checks of each mirror.
INITIAL_CHECK_DELAY = timedelta(seconds=30)
# Approximately how long to wait between checks after the initial round.
CHECK_INTERVAL = timedelta(minutes=30)
CHECK_JITTER_FRACTION = 0.05

STALENESS_LIMIT = timedelta(days=3)
CONSECUTIVE_FAIL_LIMIT = round(timedelta(days=3) / CHECK_INTERVAL)
# How long to wait before giving up on retrieving a release file, end-to-end. This
# should be generous: the `main` Release is around 13 KiB, and this timeout should
# represent a so-slow-it's-basically-stalled level of throughput.
RELEASE_RETRIEVAL_LIMIT_S = 120

REPORTING_CODE_REPO = "https://github.com/tstein/mirror-minder"

#############################
# argument-controlled state #
#############################
LOG_ONLY = False


@dataclass
class Mirror:
  """Represents a single mirror of a single package repo. Different repos (in apt terms)
  are different mirrors (in terms of this class).

  e.g., there is one Mirror object for https://packages-cf.termux.dev/apt/termux-main."""

  # Static info about the mirror.
  repo_url: str
  # The name of the package repo. e.g. main, root, x11
  repo_name: str
  weight: int

  # Mirror-checking state.
  next_check: datetime
  # Number of times in a row we've failed to successfully get the release file and parse
  # a sync time.
  consecutive_check_failures: int
  # The last time we attempted to check the mirror. None means we have never tried.
  last_check: Optional[datetime]
  # The last time we were able to get and parse the mirror's release file. None means we
  # have never done that.
  last_successful_check: Optional[datetime]
  # The last sync time reported by the last successful pull and parse of the mirror's
  # release file. None means we have never done that.
  last_sync_time: Optional[datetime]

  @property
  def domain(self) -> str:
    return self.repo_url.split("/")[2]

  def is_authoritative(self) -> bool:
    """Mirror freshness needs to be determined against an authoritative mirror, not the
    current wall time - it's normal for repos to sometimes go longer than the staleness
    limit without there being any new data.

    All mirrors are equal from a client perpective, so this program has secret knowledge
    of which mirrors are authoritative."""
    return self.repo_url.startswith("https://packages.termux.dev")

  def release_url(self) -> str:
    """Returns the full URL of the Release file for this repo."""
    repo_path = "stable" if self.repo_name == "main" else self.repo_name
    return f"{self.repo_url}/dists/{repo_path}/Release"


@dataclass
class MirrorGroup:
  """Represents a group of mirrors behind the same domain. Failures, and particularly
  failures-to-monitor, are likely to be correlated among repos hosted in the same place,
  so it's useful to keep them together."""

  domain: str
  mirrors: list[Mirror]


def clone_or_update_termux_tools_repo() -> None:
  """Call while in the workdir. We have a recent commit of termux-tools after this
  returns."""
  if os.path.exists(TERMUX_TOOLS_REPO):
    os.chdir(TERMUX_TOOLS_REPO)
    git("clean", "-dfx")
    git("pull")
  else:
    git("clone", f"https://github.com/termux/{TERMUX_TOOLS_REPO}")
    os.chdir(TERMUX_TOOLS_REPO)
  os.chdir("..")


def check_time(delay: Optional[timedelta] = None) -> datetime:
  """Chooses a time to check something. Jittered.

  If no delay is passed, defaults to the configured interval."""
  if not delay:
    delay = CHECK_INTERVAL

  # Choose a jitter factor in [-1, 1).
  jitter = ((random.random() * 2) - 1) * (delay * CHECK_JITTER_FRACTION)
  return datetime.now(UTC) + delay + jitter


def load_mirrors_from_file(domain: str, filepath: str) -> MirrorGroup:
  """Creates Mirrors for each repo in the mirror definition file at the given path."""
  mirrors: list[Mirror] = []
  repos: dict[str, str] = {}
  weight = -1
  with open(filepath) as f:
    for line in f:
      if line.strip().startswith("#"):
        continue
      var, val = line.strip().split("=")
      match var:
        case "WEIGHT":
          weight = int(val)
        case _:
          repos[var.lower()] = val.strip('"')
  for repo_name, repo_url in repos.items():
    mirrors.append(
      Mirror(
        repo_url=repo_url,
        repo_name=repo_name,
        weight=int(weight),
        next_check=check_time(INITIAL_CHECK_DELAY),
        consecutive_check_failures=0,
        last_check=None,
        last_successful_check=None,
        last_sync_time=None,
      )
    )
  logging.debug(f"loaded from {filepath}, mirrors={mirrors}")
  return MirrorGroup(domain, mirrors)


def load_mirrors_from_repo() -> list[MirrorGroup]:
  """Creates Mirrors in MirrorGroups for each (termux package) repo in the (termux-tools
  git) repo. Assumes $PWD contains the (termux-tools git) repo."""
  mirror_groups: list[MirrorGroup] = []
  mirror_dir = f"{TERMUX_TOOLS_REPO}/mirrors"
  for group in os.listdir(mirror_dir):
    group_dir = f"{mirror_dir}/{group}"
    if not os.path.isdir(group_dir):
      continue
    for domain in os.listdir(group_dir):
      mirror_file = f"{group_dir}/{domain}"
      mirror_groups.append(load_mirrors_from_file(domain, mirror_file))
  mirror_count = sum([len(m_g.mirrors) for m_g in mirror_groups])
  logging.info(
    f"loaded {len(mirror_groups)} mirror groups, {mirror_count} mirrors from repo"
  )
  logging.debug(f"mirror_groups={mirror_groups}")
  return mirror_groups


def extract_authoritative_mirrors(
  mirror_groups: list[MirrorGroup],
) -> dict[str, Mirror]:
  """Takes a list of mirror groups and extracts the authoritative mirrors for each repo.

  Returns a dict mapping repo names (e.g. "main") to the appropriate Mirrors."""
  authorities = {}
  for mirror_group in mirror_groups:
    for mirror in mirror_group.mirrors:
      if mirror.is_authoritative():
        # We need a bunch more code to do anything sensible if we believe in multiple
        # authorities.
        assert mirror.repo_name not in authorities
        authorities[mirror.repo_name] = mirror
  logging.info(f"extracted authoritative mirrors: {authorities}")
  return authorities


def check_and_update_mirror(mirror: Mirror) -> Mirror:
  """Retrieve the given mirror's release file and parse a last sync time out of it.
  Update the state in the Mirror with our success or failure. Returns the same Mirror
  object that was passed in, but only so the type checker will yell if this doesn't
  explicitly signal failure or success."""

  def fail(mirror) -> Mirror:
    mirror.last_check = datetime.now(UTC)
    mirror.consecutive_check_failures += 1
    mirror.next_check = check_time()
    return mirror

  def succeed(mirror, sync_time, release_url) -> Mirror:
    mirror.last_check = datetime.now(UTC)
    mirror.last_successful_check = datetime.now(UTC)
    mirror.last_sync_time = sync_time
    mirror.consecutive_check_failures = 0
    mirror.next_check = check_time()
    logging.info(f"successfully retrieved {release_url}")
    logging.debug(f"mirror={mirror}")
    return mirror

  release_url = mirror.release_url()
  start = time.monotonic()
  try:
    release_req = requests.get(release_url, timeout=RELEASE_RETRIEVAL_LIMIT_S)
  except requests.exceptions.ConnectionError:
    if time.monotonic() - start > RELEASE_RETRIEVAL_LIMIT_S:
      logging.error(f"connect timeout for {release_url}")
    else:
      logging.error(f"connect failure for {release_url}")
    logging.debug(f"mirror={mirror}")
    return fail(mirror)
  except (requests.exceptions.ReadTimeout, urllib3.exceptions.ReadTimeoutError):
    logging.error(f"read timeout for {release_url}")
    return fail(mirror)
  if release_req.status_code != 200:
    logging.warning(f"retrieving {release_url} returned HTTP {release_req.status_code}")
    logging.debug(f"mirror={mirror}")
    return fail(mirror)

  for line in release_req.text.splitlines():
    if line.startswith("Date:"):
      # This is a sync time. line looks like this: "Date: Wed, 28 May 2025 06:20:22 UTC"
      # Assumes:
      #   it's always UTC, which might not be true
      #   it uses abbreviated month names, which isn't easy to confirm because I'm
      #     writing this in May
      try:
        sync_time_str = line.strip("Date: ").rstrip(" UTC").split(", ")[1]
        sync_time = datetime.fromtimestamp(
          datetime.strptime(sync_time_str, "%d %b %Y %H:%M:%S").timestamp(), UTC
        )
        return succeed(mirror, sync_time, release_url)
      except ValueError:
        logging.exception(
          f"retrieved release file at {release_url}, but couldn't parse the sync time"
        )
        logging.debug(f"mirror={mirror}")
        return fail(mirror)

  # We didn't find a sync time in the release file. Treat this as a failure.
  logging.error(
    f"retrieved release file at {release_url}, but couldn't find a sync time"
  )
  logging.debug(f"mirror={mirror}")
  return fail(mirror)


def file_github_issue(repo_domain: str, details: str) -> None:
  """Create an issue in the configured repo, if there isn't already an open one for this
  repo."""
  title = issue_title(repo_domain)
  body = issue_body(repo_domain, details)
  if LOG_ONLY:
    logging.warning(f"would create issue, but running log-only:\n{title}\n{body}")
    return

  try:
    if url := search_issues(REPORTING_CODE_REPO, title):
      logging.info(f"found existing issue: {url}")
    else:
      issue_url = open_new_issue(REPORTING_CODE_REPO, title, body)
      logging.warning(f"created issue {issue_url}")
  except (ValueError, sh.ErrorReturnCode):
    logging.exception(
      "something went wrong communicating with github - no issue created"
    )


def judge_mirror(mirror: Mirror, authority: Optional[Mirror]) -> tuple[bool, str]:
  """Decide if a mirror looks unhealthy and return an explanation of why or why not."""
  # If we're trying to judge an authority, it's a bug.
  assert not mirror.is_authoritative()

  # We need to know if we are failing to monitor the mirror.
  if mirror.consecutive_check_failures > CONSECUTIVE_FAIL_LIMIT:
    return (
      False,
      f"checking it failed {mirror.consecutive_check_failures} times in a row",
    )

  # Failure to determine the sync time is counted in consecutive failures, and authority
  # freshness needs to be handled separately. It's okay do nothing if we don't have
  # enough info to do anything else where.
  if not mirror.last_sync_time or not authority or not authority.last_sync_time:
    return True, "no authority available and failure/staleness limits not yet exceeded"

  # This is the freshness check we're all here for.
  if authority.last_sync_time - mirror.last_sync_time > STALENESS_LIMIT:
    return False, f"hasn't synced since {mirror.last_sync_time}"

  return True, f"looks good, last synced {mirror.last_sync_time}"


def judge_mirror_group(group: MirrorGroup, authorities: dict[str, Mirror]) -> None:
  """Decide if a mirror group looks unhealthy and do something useful if it does.

  Does nothing if we haven't at least tried to check every mirror in the group at least
  once."""
  if not all([m.last_check for m in group.mirrors]):
    logging.info(
      f"not judging mirror group at {group.domain} until all mirrors are checked at least once"
    )
    return

  group_healthy = True
  explanations: list[tuple[Mirror, str]] = []
  for mirror in group.mirrors:
    # This path judges mirrors against authorities. It does not handle authority issues.
    if mirror.is_authoritative():
      continue
    mirror_healthy, explanation = judge_mirror(
      mirror, authorities.get(mirror.repo_name)
    )
    group_healthy = group_healthy and mirror_healthy
    explanations.append((mirror, explanation))
  if group_healthy:
    return

  def p(mirror, explanation):
    return f"## {mirror.repo_name}\n\n{explanation}\n"

  detail_parts = [p(m, e) for (m, e) in explanations]
  details = "\n==========\n".join(detail_parts)

  file_github_issue(group.domain, details)


def check_mirrors_forever(
  mirror_groups: list[MirrorGroup], authorities: dict[str, Mirror]
) -> None:
  while True:
    time.sleep(0.1)
    now = datetime.now(UTC)
    for group in mirror_groups:
      did_anything = False
      for mirror in group.mirrors:
        if mirror.next_check < now:
          _ = check_and_update_mirror(mirror)
          did_anything = True

      if did_anything:
        judge_mirror_group(group, authorities)


def main() -> None:
  global LOG_ONLY

  logging.basicConfig(level=logging.INFO)

  parser = argparse.ArgumentParser()
  parser.add_argument(
    "--log-only",
    action="store_true",
    default=False,
    help="Don't touch github. If a repo is bad, just log.",
  )
  parser.add_argument("WORKDIR")
  args = parser.parse_args()
  if args.log_only:
    logging.warning("running in log-only mode")
    LOG_ONLY = True
  os.chdir(args.WORKDIR)

  clone_or_update_termux_tools_repo()
  mirror_groups = load_mirrors_from_repo()
  authorities = extract_authoritative_mirrors(mirror_groups)

  # Authorities remain in the all-mirrors list so we can monitor them with the same
  # logic as secondaries, but we can avoid some log noise by making sure they're checked
  # first.
  for group in mirror_groups:
    for mirror in group.mirrors:
      if mirror.is_authoritative():
        mirror.next_check = datetime.fromtimestamp(0, UTC)
  check_mirrors_forever(mirror_groups, authorities)


if __name__ == "__main__":
  try:
    main()
  except KeyboardInterrupt:
    pass
